import streamlit as st
import pandas as pd
import torch
import joblib
import pandas as pd
import sys
from transformers import(
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig, # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì–‘ìí™”
    pipeline
)
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import LabelEncoder
from SelectQuestion import SelectOneQuestion # ì§ì ‘ ë§Œë“  í´ë˜ìŠ¤
from sklearn.metrics.pairwise import cosine_similarity

# í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ ê¸°ë³¸ ì„¤ì • 
# í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model = AutoModelForCausalLM.from_pretrained("/content/drive/MyDrive/full_trained_model", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/full_trained_model")

# íŠ¹ì • ë””ë ‰í† ë¦¬ë¥¼ ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œì— ì¶”ê°€
sys.path.append('/content/drive/MyDrive/healthcare project')

# í•™ìŠµëœ ë¶„ë¥˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
rf_model = joblib.load('/content/drive/MyDrive/healthcare project/headache_randomforest.pkl')
scaler = joblib.load('/content/drive/MyDrive/healthcare project/headache_scaler.pkl')
pca = joblib.load('/content/drive/MyDrive/healthcare project/headache_pca.pkl')

# í´ë˜ìŠ¤ ë™ì‘ì„ ìœ„í•œ ì •ì˜
question_df = pd.read_csv("/content/drive/MyDrive/healthcare project/á„ƒá…®á„á…©á†¼ á„Œá…µá†¯á„†á…®á†«.csv", index_col=0)
embed_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
labelencoder = LabelEncoder()

# ê°ì²´ ìƒì„±
select_question = SelectOneQuestion(question_df, scaler, pca, rf_model, embed_model, labelencoder)
# ë²¡í„° DB ë§Œë“¤ê¸°
intention_map, question_map = select_question.make_vec_db()


# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ êµ¬í•˜ëŠ” í•¨ìˆ˜
def krsbert_similarity(question, result_question):

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    user_question_embedding = embed_model.encode(question) # ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©
    select_question_embedding = embed_model.encode(result_question) # ì¶”ì¶œëœ ì§ˆë¬¸ ì„ë² ë”©
    cos_sim = cosine_similarity(user_question_embedding.reshape(1,-1), select_question_embedding.reshape(1,-1)) # ì½”ì‚¬ì¸ ìœ ì‚¬ë„

    return cos_sim[0][0]

# íŒŒì´í”„ë¼ì¸ ë¯¸ë¦¬ ìƒì„±(ë‹µë³€ ì¶œë ¥í•  ë•Œ ë§ˆë‹¤ ìƒì„±í•˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•¨.)
def load_pipe(model, tokenizer):
    return pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=256)

# ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer(user_input):

    question = user_input
    result_question = select_question.search_one_question(question, intention_map, question_map)

    # ìœ ì‚¬ë„ ì ìˆ˜ 0.84 ì´ìƒì˜ ì§ˆë¬¸ë“¤ë§Œ
    if krsbert_similarity(question, result_question).round(2) >= 0.84:
        filtered_question = result_question

    else:
        filtered_question = question

    # ë‹µë³€ ìƒì„±
    query = filtered_question
    pipe = load_pipe(model, tokenizer)
    result = pipe(f"<s>[INST]{query}[/INST]")
    answer_result = result[0]['generated_text']
    answer = answer_result.split('[/INST]')[1].split('<')[0]

    # ë§ˆì¹¨í‘œë¡œ ì•ˆëë‚˜ëŠ” ê²½ìš° ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œê¹Œì§€ë§Œ ì¶œë ¥ë ¥
    result = answer[:answer.rfind('.')+1]

    return result

    

st.title("ğŸ§  ì˜ë£Œ AI ì±—ë´‡")

with st.chat_message("assistant"):
    st.write("ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ë¶€ë¶„ì„ ë§ì”€í•´ì£¼ì„¸ìš”.ğŸ‘‹")

prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
if prompt:
    with st.chat_message("user"):
        st.write(prompt)

    response = generate_answer(prompt)

    # ğŸ¤– ì±—ë´‡ ë©”ì‹œì§€ ì¶œë ¥
    with st.chat_message("assistant"):
        st.write(response)