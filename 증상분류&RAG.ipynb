{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 입력한 질문 벡터화 및 차원축소 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# KoSentence-BERT 모델 불러오기\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "question = '오른쪽 머리에서 찌릿하는 두통은 뭔가요?'\n",
    "question_vec = model.encode([question], batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 저장된 RandomForestClassifier 모델 불러오기\n",
    "rf_model = joblib.load('./headache_randomforest.pkl')\n",
    "scaler = joblib.load('./headache_scaler.pkl')\n",
    "pca = joblib.load('./headache_pca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "question_ss = scaler.transform(question_vec)\n",
    "question_pca = pca.transform(question_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "result = rf_model.predict(question_pca)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 질문 데이터 불러오기\n",
    "question_df = pd.read_csv('두통 질문.csv', index_col=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le_intention = le.fit_transform(question_df['intention'])\n",
    "le_intention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['증상'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = le.inverse_transform([result])\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. faiss\n",
    "- 대규모 데이터셋에서 빠르고 효율적으로 문장 유사도 검색.\n",
    "- faiss로 벡터 DB 구축\n",
    "- 사용자가 입력한 질문을 벡터화하고 faiss를 통해 가장 유사한 벡터 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 임베딩\n",
    "#### 1) 텍스트 나누기\n",
    "- 각 언어 모델은 한 번에 처리할 수 있는 텍스트의 길이(토큰수)가 제한적이다.\n",
    "- 한번에 처리하는 텍스트 길이에 제한을 두어 큰 텍스트를 처리할 때 효율적으로 하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오른쪽 머리에서 찌릿하는 두통은 뭔가요?',\n",
       " '두달에 한 번 정도 머리가 아픈데 도대체 뭐 때문에 아픈지 모르겠습니다.',\n",
       " '심각한 문제일까요?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = '오른쪽 머리에서 찌릿하는 두통은 뭔가요? 두달에 한 번 정도 머리가 아픈데 도대체 뭐 때문에 아픈지 모르겠습니다. 심각한 문제일까요?'\n",
    "re.split(r'(?<=[.!?]) +', test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2문장씩 문장을 나누는 함수\n",
    "def split_sentences(sentences, max_sentences=2):\n",
    "    sentence_list = re.split(r'(?<=[.!?]) +', sentences) # 문장을 마침표, 느낌표, 물음표 뒤에오는 공백을 기준으로 나눔(공백 뒤에 +를 붙여서 하나이상을 의미)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentence_list), max_sentences): # max_sentences 간격만큼\n",
    "        chunks.append(' '.join(sentence_list[i:i + max_sentences])) # join으로 두개의 문장 공백으로 연결, 리스트 대신 문자열로 생성됨.\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오른쪽 머리에서 찌릿하는 두통은 뭔가요? 두달에 한 번 정도 머리가 아픈데 도대체 뭐 때문에 아픈지 모르겠습니다.',\n",
       " '심각한 문제일까요?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentences(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks 속 2문장씩 결합되어있는 문장들의 길이가 너무 긴 경우 나눠주기 위함.\n",
    "def split_text_with_recursive_splitter(text, chunk_size=300, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, # 최대 몇자로 설정할지\n",
    "        chunk_overlap = chunk_overlap # 50개 겹쳐서 시작\n",
    "    )\n",
    "\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오른쪽 머리에서 찌릿하는 두통은 뭔가요? 두달에 한 번 정도 머리가 아픈데 도대체 뭐 때문에 아픈지 모르겠습니다. 심각한 문제일까요?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_with_recursive_splitter(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease_category</th>\n",
       "      <th>disease_name</th>\n",
       "      <th>intention</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>예방</td>\n",
       "      <td>두통 예방을 위해 어떤 자세나 활동이 도움이 될까요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>예방</td>\n",
       "      <td>스트레스를 많이 받는 사람이 두통을 예방하기 위해 어떤 생활습관을 가져야 할까요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>예방</td>\n",
       "      <td>대상포진 예방접종 후에 어지럼증과 두통이 예방접종과 관련이 있다면 어떻게 되나요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>예방</td>\n",
       "      <td>갈색세포증과 관련하여 일상생활에서 신경써야할 점은 무엇인가요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>예방</td>\n",
       "      <td>두통을 예방하기 위한 수면 자세에는 어떤 것들이 있을까요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>치료</td>\n",
       "      <td>두통과 스트레스 반응에 대한 급성 과민성을 진단하고 치료하는 방법에는 어떤 것들이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>치료</td>\n",
       "      <td>약국에서 구입 가능한 두통 치료제 중에서 어떤 것이 가장 효과적인지 알려주세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>치료</td>\n",
       "      <td>만성 두통을 치료하기 위해 어떤 방법이 있는지 알려주세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>치료</td>\n",
       "      <td>두통의 치료를 위해 어떤 약물이 사용될 수 있을까요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>응급질환</td>\n",
       "      <td>두통</td>\n",
       "      <td>치료</td>\n",
       "      <td>두통이 자주 생기는데, 이를 치료하기 위해 어떤 치료법을 사용할 수 있을까요?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3428 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     disease_category disease_name intention  \\\n",
       "0                응급질환           두통        예방   \n",
       "1                응급질환           두통        예방   \n",
       "2                응급질환           두통        예방   \n",
       "3                응급질환           두통        예방   \n",
       "4                응급질환           두통        예방   \n",
       "...               ...          ...       ...   \n",
       "3423             응급질환           두통        치료   \n",
       "3424             응급질환           두통        치료   \n",
       "3425             응급질환           두통        치료   \n",
       "3426             응급질환           두통        치료   \n",
       "3427             응급질환           두통        치료   \n",
       "\n",
       "                                               question  \n",
       "0                         두통 예방을 위해 어떤 자세나 활동이 도움이 될까요?  \n",
       "1         스트레스를 많이 받는 사람이 두통을 예방하기 위해 어떤 생활습관을 가져야 할까요?  \n",
       "2         대상포진 예방접종 후에 어지럼증과 두통이 예방접종과 관련이 있다면 어떻게 되나요?  \n",
       "3                    갈색세포증과 관련하여 일상생활에서 신경써야할 점은 무엇인가요?  \n",
       "4                      두통을 예방하기 위한 수면 자세에는 어떤 것들이 있을까요?  \n",
       "...                                                 ...  \n",
       "3423  두통과 스트레스 반응에 대한 급성 과민성을 진단하고 치료하는 방법에는 어떤 것들이 ...  \n",
       "3424       약국에서 구입 가능한 두통 치료제 중에서 어떤 것이 가장 효과적인지 알려주세요.  \n",
       "3425                   만성 두통을 치료하기 위해 어떤 방법이 있는지 알려주세요.  \n",
       "3426                      두통의 치료를 위해 어떤 약물이 사용될 수 있을까요?  \n",
       "3427        두통이 자주 생기는데, 이를 치료하기 위해 어떤 치료법을 사용할 수 있을까요?  \n",
       "\n",
       "[3428 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문 불러오기\n",
    "question_df = pd.read_csv('두통 질문.csv', index_col=0)\n",
    "question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = question_df['question'].tolist()\n",
    "intention_list = question_df['intention'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하이요', '하이요']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = []\n",
    "a='하이요'\n",
    "[a]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하이요', '하이요']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list.extend([a]*2)\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunk = []\n",
    "question_map = []\n",
    "intention_map = []\n",
    "\n",
    "for question in question_list:\n",
    "    # 2문장씩 나누기\n",
    "    sentence_chunk = split_sentences(question, max_sentences=2)\n",
    "\n",
    "    # 긴 문장 쪼개기\n",
    "    for chunk in sentence_chunk:\n",
    "        split_chunks = split_text_with_recursive_splitter(chunk, chunk_size=300, chunk_overlap=50)\n",
    "        final_chunk.extend(split_chunks)\n",
    "\n",
    "        # 분할된 chunk마다 원본 맵핑\n",
    "        question_map.extend([question] * len(split_chunks))\n",
    "        # 분할된 chunk마다 질문 의도 맵핑\n",
    "        intention = question_df[question_df['question'] == question]['intention'].values[0] # series 형태여서 values로 텍스트만 가져옴.\n",
    "        intention_map.extend([intention] * len(split_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['두통 예방을 위해 어떤 자세나 활동이 도움이 될까요?',\n",
       " '스트레스를 많이 받는 사람이 두통을 예방하기 위해 어떤 생활습관을 가져야 할까요?',\n",
       " '대상포진 예방접종 후에 어지럼증과 두통이 예방접종과 관련이 있다면 어떻게 되나요?',\n",
       " '갈색세포증과 관련하여 일상생활에서 신경써야할 점은 무엇인가요?',\n",
       " '두통을 예방하기 위한 수면 자세에는 어떤 것들이 있을까요?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_map[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'원인'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intention_map[387]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 질문 임베딩 및 질문 벡터 DB 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 임베딩 개수:  3428\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "embeddings = model.encode(final_chunk)\n",
    "print('생성된 임베딩 개수: ', len(embeddings)) # 질문은 짧아서 임베딩 개수와 실제 데이터 개수가 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3428, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 질문 벡터 DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스에 저장된 벡터 개수: 3428\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# faiss 인덱스 생성 및 데이터 삽입\n",
    "dimension = embeddings.shape[1] # 임베딩 벡터의 차원\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 거리(유클리드 거리) 기반으로 검색을 수행하도록 설정(인덱스 생성)\n",
    "index.add(np.array(embeddings)) # 각 벡터를 L2 거리 기반 인덱스에 추가하고, 검색을 빠르게 할 수 있도록 데이터 구조를 최적화\n",
    "\n",
    "# 디버깅용 출력: 저장된 벡터 개수 확인\n",
    "print(f\"FAISS 인덱스에 저장된 벡터 개수: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터 DB에서 검색\n",
    "1) 질문을 받고 의도를 예측한다 (학습시킨 분류 모델 사용)\n",
    "2) 의도에 해당하는 질문 DB만 추출\n",
    "3) 임베딩한 질문과 가장 유사한 질문을 추출한 질문 벡터 DB에서 찾는다(faiss 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) 질문 의도 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '오른쪽 머리에서 찌릿하는 두통은 뭔가요?'\n",
    "question_vec = model.encode([question], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_ss = scaler.transform(question_vec)\n",
    "question_pca = pca.transform(question_ss)\n",
    "result = rf_model.predict(question_pca)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'증상'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = le.inverse_transform([result])\n",
    "label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) 의도에 해당하는 질문 DB만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767,\n",
       " [1481,\n",
       "  1482,\n",
       "  1483,\n",
       "  1484,\n",
       "  1485,\n",
       "  1486,\n",
       "  1487,\n",
       "  1488,\n",
       "  1489,\n",
       "  1490,\n",
       "  1491,\n",
       "  1492,\n",
       "  1493,\n",
       "  1494,\n",
       "  1495,\n",
       "  1496,\n",
       "  1497,\n",
       "  1498,\n",
       "  1499,\n",
       "  1500,\n",
       "  1501,\n",
       "  1502,\n",
       "  1503,\n",
       "  1504,\n",
       "  1505,\n",
       "  1506,\n",
       "  1507,\n",
       "  1508,\n",
       "  1509,\n",
       "  1510,\n",
       "  1511,\n",
       "  1512,\n",
       "  1513,\n",
       "  1514,\n",
       "  1515,\n",
       "  1516,\n",
       "  1517,\n",
       "  1518,\n",
       "  1519,\n",
       "  1520,\n",
       "  1521,\n",
       "  1522,\n",
       "  1523,\n",
       "  1524,\n",
       "  1525,\n",
       "  1526,\n",
       "  1527,\n",
       "  1528,\n",
       "  1529,\n",
       "  1530,\n",
       "  1531,\n",
       "  1532,\n",
       "  1533,\n",
       "  1534,\n",
       "  1535,\n",
       "  1536,\n",
       "  1537,\n",
       "  1538,\n",
       "  1539,\n",
       "  1540,\n",
       "  1541,\n",
       "  1542,\n",
       "  1543,\n",
       "  1544,\n",
       "  1545,\n",
       "  1546,\n",
       "  1547,\n",
       "  1548,\n",
       "  1549,\n",
       "  1550,\n",
       "  1551,\n",
       "  1552,\n",
       "  1553,\n",
       "  1554,\n",
       "  1555,\n",
       "  1556,\n",
       "  1557,\n",
       "  1558,\n",
       "  1559,\n",
       "  1560,\n",
       "  1561,\n",
       "  1562,\n",
       "  1563,\n",
       "  1564,\n",
       "  1565,\n",
       "  1566,\n",
       "  1567,\n",
       "  1568,\n",
       "  1569,\n",
       "  1570,\n",
       "  1571,\n",
       "  1572,\n",
       "  1573,\n",
       "  1574,\n",
       "  1575,\n",
       "  1576,\n",
       "  1577,\n",
       "  1578,\n",
       "  1579,\n",
       "  1580,\n",
       "  1581,\n",
       "  1582,\n",
       "  1583,\n",
       "  1584,\n",
       "  1585,\n",
       "  1586,\n",
       "  1587,\n",
       "  1588,\n",
       "  1589,\n",
       "  1590,\n",
       "  1591,\n",
       "  1592,\n",
       "  1593,\n",
       "  1594,\n",
       "  1595,\n",
       "  1596,\n",
       "  1597,\n",
       "  1598,\n",
       "  1599,\n",
       "  1600,\n",
       "  1601,\n",
       "  1602,\n",
       "  1603,\n",
       "  1604,\n",
       "  1605,\n",
       "  1606,\n",
       "  1607,\n",
       "  1608,\n",
       "  1609,\n",
       "  1610,\n",
       "  1611,\n",
       "  1612,\n",
       "  1613,\n",
       "  1614,\n",
       "  1615,\n",
       "  1616,\n",
       "  1617,\n",
       "  1618,\n",
       "  1619,\n",
       "  1620,\n",
       "  1621,\n",
       "  1622,\n",
       "  1623,\n",
       "  1624,\n",
       "  1625,\n",
       "  1626,\n",
       "  1627,\n",
       "  1628,\n",
       "  1629,\n",
       "  1630,\n",
       "  1631,\n",
       "  1632,\n",
       "  1633,\n",
       "  1634,\n",
       "  1635,\n",
       "  1636,\n",
       "  1637,\n",
       "  1638,\n",
       "  1639,\n",
       "  1640,\n",
       "  1641,\n",
       "  1642,\n",
       "  1643,\n",
       "  1644,\n",
       "  1645,\n",
       "  1646,\n",
       "  1647,\n",
       "  1648,\n",
       "  1649,\n",
       "  1650,\n",
       "  1651,\n",
       "  1652,\n",
       "  1653,\n",
       "  1654,\n",
       "  1655,\n",
       "  1656,\n",
       "  1657,\n",
       "  1658,\n",
       "  1659,\n",
       "  1660,\n",
       "  1661,\n",
       "  1662,\n",
       "  1663,\n",
       "  1664,\n",
       "  1665,\n",
       "  1666,\n",
       "  1667,\n",
       "  1668,\n",
       "  1669,\n",
       "  1670,\n",
       "  1671,\n",
       "  1672,\n",
       "  1673,\n",
       "  1674,\n",
       "  1675,\n",
       "  1676,\n",
       "  1677,\n",
       "  1678,\n",
       "  1679,\n",
       "  1680,\n",
       "  1681,\n",
       "  1682,\n",
       "  1683,\n",
       "  1684,\n",
       "  1685,\n",
       "  1686,\n",
       "  1687,\n",
       "  1688,\n",
       "  1689,\n",
       "  1690,\n",
       "  1691,\n",
       "  1692,\n",
       "  1693,\n",
       "  1694,\n",
       "  1695,\n",
       "  1696,\n",
       "  1697,\n",
       "  1698,\n",
       "  1699,\n",
       "  1700,\n",
       "  1701,\n",
       "  1702,\n",
       "  1703,\n",
       "  1704,\n",
       "  1705,\n",
       "  1706,\n",
       "  1707,\n",
       "  1708,\n",
       "  1709,\n",
       "  1710,\n",
       "  1711,\n",
       "  1712,\n",
       "  1713,\n",
       "  1714,\n",
       "  1715,\n",
       "  1716,\n",
       "  1717,\n",
       "  1718,\n",
       "  1720,\n",
       "  1721,\n",
       "  1722,\n",
       "  1723,\n",
       "  1724,\n",
       "  1725,\n",
       "  1726,\n",
       "  1727,\n",
       "  1728,\n",
       "  1729,\n",
       "  1730,\n",
       "  1731,\n",
       "  1732,\n",
       "  1733,\n",
       "  1734,\n",
       "  1735,\n",
       "  1736,\n",
       "  1737,\n",
       "  1738,\n",
       "  1739,\n",
       "  1740,\n",
       "  1741,\n",
       "  1742,\n",
       "  1743,\n",
       "  1744,\n",
       "  1745,\n",
       "  1746,\n",
       "  1747,\n",
       "  1748,\n",
       "  1749,\n",
       "  1750,\n",
       "  1751,\n",
       "  1752,\n",
       "  1753,\n",
       "  1754,\n",
       "  1755,\n",
       "  1756,\n",
       "  1757,\n",
       "  1758,\n",
       "  1759,\n",
       "  1760,\n",
       "  1761,\n",
       "  1762,\n",
       "  1763,\n",
       "  1765,\n",
       "  1766,\n",
       "  1767,\n",
       "  1768,\n",
       "  1769,\n",
       "  1770,\n",
       "  1771,\n",
       "  1772,\n",
       "  1773,\n",
       "  1774,\n",
       "  1775,\n",
       "  1776,\n",
       "  1777,\n",
       "  1778,\n",
       "  1779,\n",
       "  1780,\n",
       "  1781,\n",
       "  1782,\n",
       "  1783,\n",
       "  1784,\n",
       "  1785,\n",
       "  1786,\n",
       "  1787,\n",
       "  1788,\n",
       "  1789,\n",
       "  1790,\n",
       "  1791,\n",
       "  1792,\n",
       "  1793,\n",
       "  1794,\n",
       "  1795,\n",
       "  1796,\n",
       "  1797,\n",
       "  1798,\n",
       "  1799,\n",
       "  1800,\n",
       "  1801,\n",
       "  1802,\n",
       "  1803,\n",
       "  1804,\n",
       "  1805,\n",
       "  1806,\n",
       "  1807,\n",
       "  1808,\n",
       "  1809,\n",
       "  1810,\n",
       "  1811,\n",
       "  1812,\n",
       "  1813,\n",
       "  1814,\n",
       "  1815,\n",
       "  1816,\n",
       "  1817,\n",
       "  1818,\n",
       "  1819,\n",
       "  1820,\n",
       "  1821,\n",
       "  1822,\n",
       "  1823,\n",
       "  1824,\n",
       "  1825,\n",
       "  1826,\n",
       "  1828,\n",
       "  1829,\n",
       "  1830,\n",
       "  1831,\n",
       "  1832,\n",
       "  1833,\n",
       "  1834,\n",
       "  1835,\n",
       "  1836,\n",
       "  1837,\n",
       "  1838,\n",
       "  1839,\n",
       "  1840,\n",
       "  1841,\n",
       "  1842,\n",
       "  1843,\n",
       "  1844,\n",
       "  1845,\n",
       "  1846,\n",
       "  1847,\n",
       "  1848,\n",
       "  1849,\n",
       "  1850,\n",
       "  1851,\n",
       "  1852,\n",
       "  1853,\n",
       "  1854,\n",
       "  1855,\n",
       "  1856,\n",
       "  1857,\n",
       "  1858,\n",
       "  1859,\n",
       "  1860,\n",
       "  1861,\n",
       "  1862,\n",
       "  1863,\n",
       "  1864,\n",
       "  1865,\n",
       "  1866,\n",
       "  1867,\n",
       "  1868,\n",
       "  1869,\n",
       "  1870,\n",
       "  1871,\n",
       "  1872,\n",
       "  1873,\n",
       "  1874,\n",
       "  1875,\n",
       "  1876,\n",
       "  1877,\n",
       "  1878,\n",
       "  1879,\n",
       "  1880,\n",
       "  1881,\n",
       "  1882,\n",
       "  1883,\n",
       "  1884,\n",
       "  1885,\n",
       "  1886,\n",
       "  1887,\n",
       "  1888,\n",
       "  1889,\n",
       "  1890,\n",
       "  1891,\n",
       "  1892,\n",
       "  1893,\n",
       "  1894,\n",
       "  1895,\n",
       "  1896,\n",
       "  1897,\n",
       "  1898,\n",
       "  1899,\n",
       "  1900,\n",
       "  1901,\n",
       "  1902,\n",
       "  1903,\n",
       "  1904,\n",
       "  1905,\n",
       "  1906,\n",
       "  1907,\n",
       "  1908,\n",
       "  1909,\n",
       "  1910,\n",
       "  1911,\n",
       "  1912,\n",
       "  1913,\n",
       "  1914,\n",
       "  1915,\n",
       "  1916,\n",
       "  1917,\n",
       "  1918,\n",
       "  1919,\n",
       "  1920,\n",
       "  1921,\n",
       "  1922,\n",
       "  1923,\n",
       "  1924,\n",
       "  1925,\n",
       "  1926,\n",
       "  1927,\n",
       "  1928,\n",
       "  1929,\n",
       "  1930,\n",
       "  1931,\n",
       "  1932,\n",
       "  1933,\n",
       "  1934,\n",
       "  1935,\n",
       "  1936,\n",
       "  1937,\n",
       "  1938,\n",
       "  1939,\n",
       "  1940,\n",
       "  1941,\n",
       "  1942,\n",
       "  1943,\n",
       "  1944,\n",
       "  1945,\n",
       "  1946,\n",
       "  1947,\n",
       "  1948,\n",
       "  1949,\n",
       "  1950,\n",
       "  1951,\n",
       "  1952,\n",
       "  1953,\n",
       "  1954,\n",
       "  1955,\n",
       "  1956,\n",
       "  1957,\n",
       "  1958,\n",
       "  1959,\n",
       "  1960,\n",
       "  1961,\n",
       "  1962,\n",
       "  1963,\n",
       "  1964,\n",
       "  1965,\n",
       "  1966,\n",
       "  1967,\n",
       "  1968,\n",
       "  1969,\n",
       "  1970,\n",
       "  1971,\n",
       "  1972,\n",
       "  1973,\n",
       "  1974,\n",
       "  1975,\n",
       "  1976,\n",
       "  1977,\n",
       "  1978,\n",
       "  1979,\n",
       "  1980,\n",
       "  1981,\n",
       "  1982,\n",
       "  1983,\n",
       "  1984,\n",
       "  1985,\n",
       "  1986,\n",
       "  1987,\n",
       "  1988,\n",
       "  1989,\n",
       "  1990,\n",
       "  1991,\n",
       "  1992,\n",
       "  1993,\n",
       "  1994,\n",
       "  1995,\n",
       "  1996,\n",
       "  1997,\n",
       "  1998,\n",
       "  1999,\n",
       "  2000,\n",
       "  2001,\n",
       "  2002,\n",
       "  2003,\n",
       "  2004,\n",
       "  2005,\n",
       "  2006,\n",
       "  2007,\n",
       "  2008,\n",
       "  2009,\n",
       "  2010,\n",
       "  2011,\n",
       "  2012,\n",
       "  2013,\n",
       "  2014,\n",
       "  2015,\n",
       "  2016,\n",
       "  2017,\n",
       "  2018,\n",
       "  2019,\n",
       "  2020,\n",
       "  2021,\n",
       "  2022,\n",
       "  2023,\n",
       "  2024,\n",
       "  2025,\n",
       "  2026,\n",
       "  2027,\n",
       "  2028,\n",
       "  2029,\n",
       "  2030,\n",
       "  2031,\n",
       "  2032,\n",
       "  2034,\n",
       "  2035,\n",
       "  2036,\n",
       "  2037,\n",
       "  2038,\n",
       "  2039,\n",
       "  2040,\n",
       "  2041,\n",
       "  2042,\n",
       "  2043,\n",
       "  2044,\n",
       "  2045,\n",
       "  2046,\n",
       "  2047,\n",
       "  2048,\n",
       "  2049,\n",
       "  2050,\n",
       "  2051,\n",
       "  2052,\n",
       "  2053,\n",
       "  2054,\n",
       "  2055,\n",
       "  2056,\n",
       "  2057,\n",
       "  2058,\n",
       "  2059,\n",
       "  2060,\n",
       "  2061,\n",
       "  2062,\n",
       "  2063,\n",
       "  2064,\n",
       "  2065,\n",
       "  2066,\n",
       "  2067,\n",
       "  2068,\n",
       "  2069,\n",
       "  2070,\n",
       "  2071,\n",
       "  2072,\n",
       "  2073,\n",
       "  2074,\n",
       "  2075,\n",
       "  2076,\n",
       "  2077,\n",
       "  2078,\n",
       "  2079,\n",
       "  2080,\n",
       "  2081,\n",
       "  2082,\n",
       "  2083,\n",
       "  2084,\n",
       "  2085,\n",
       "  2086,\n",
       "  2087,\n",
       "  2088,\n",
       "  2089,\n",
       "  2090,\n",
       "  2091,\n",
       "  2092,\n",
       "  2093,\n",
       "  2094,\n",
       "  2095,\n",
       "  2096,\n",
       "  2097,\n",
       "  2098,\n",
       "  2099,\n",
       "  2100,\n",
       "  2101,\n",
       "  2102,\n",
       "  2103,\n",
       "  2104,\n",
       "  2105,\n",
       "  2106,\n",
       "  2107,\n",
       "  2108,\n",
       "  2109,\n",
       "  2110,\n",
       "  2111,\n",
       "  2112,\n",
       "  2113,\n",
       "  2114,\n",
       "  2115,\n",
       "  2116,\n",
       "  2117,\n",
       "  2118,\n",
       "  2119,\n",
       "  2120,\n",
       "  2121,\n",
       "  2122,\n",
       "  2123,\n",
       "  2124,\n",
       "  2125,\n",
       "  2126,\n",
       "  2127,\n",
       "  2128,\n",
       "  2129,\n",
       "  2130,\n",
       "  2131,\n",
       "  2132,\n",
       "  2133,\n",
       "  2134,\n",
       "  2135,\n",
       "  2136,\n",
       "  2137,\n",
       "  2138,\n",
       "  2139,\n",
       "  2140,\n",
       "  2141,\n",
       "  2142,\n",
       "  2143,\n",
       "  2144,\n",
       "  2145,\n",
       "  2146,\n",
       "  2147,\n",
       "  2148,\n",
       "  2149,\n",
       "  2150,\n",
       "  2151,\n",
       "  2152,\n",
       "  2153,\n",
       "  2154,\n",
       "  2155,\n",
       "  2156,\n",
       "  2157,\n",
       "  2158,\n",
       "  2159,\n",
       "  2160,\n",
       "  2161,\n",
       "  2162,\n",
       "  2163,\n",
       "  2164,\n",
       "  2165,\n",
       "  2166,\n",
       "  2167,\n",
       "  2168,\n",
       "  2169,\n",
       "  2170,\n",
       "  2171,\n",
       "  2172,\n",
       "  2173,\n",
       "  2174,\n",
       "  2175,\n",
       "  2176,\n",
       "  2177,\n",
       "  2178,\n",
       "  2180,\n",
       "  2181,\n",
       "  2182,\n",
       "  2183,\n",
       "  2184,\n",
       "  2185,\n",
       "  2186,\n",
       "  2187,\n",
       "  2188,\n",
       "  2189,\n",
       "  2190,\n",
       "  2191,\n",
       "  2192,\n",
       "  2193,\n",
       "  2194,\n",
       "  2195,\n",
       "  2196,\n",
       "  2197,\n",
       "  2198,\n",
       "  2199,\n",
       "  2200,\n",
       "  2201,\n",
       "  2202,\n",
       "  2203,\n",
       "  2204,\n",
       "  2205,\n",
       "  2206,\n",
       "  2207,\n",
       "  2208,\n",
       "  2209,\n",
       "  2210,\n",
       "  2211,\n",
       "  2212,\n",
       "  2213,\n",
       "  2214,\n",
       "  2215,\n",
       "  2216,\n",
       "  2217,\n",
       "  2218,\n",
       "  2219,\n",
       "  2220,\n",
       "  2221,\n",
       "  2222,\n",
       "  2223,\n",
       "  2224,\n",
       "  2225,\n",
       "  2226,\n",
       "  2227,\n",
       "  2228,\n",
       "  2229,\n",
       "  2230,\n",
       "  2231,\n",
       "  2232,\n",
       "  2233,\n",
       "  2234,\n",
       "  2235,\n",
       "  2236,\n",
       "  2237,\n",
       "  2238,\n",
       "  2239,\n",
       "  2240,\n",
       "  2241,\n",
       "  2242,\n",
       "  2243,\n",
       "  2244,\n",
       "  2245,\n",
       "  2246,\n",
       "  2247,\n",
       "  2248,\n",
       "  2249,\n",
       "  2250,\n",
       "  2251,\n",
       "  2252])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 결과와 같은 증상의 인덱스만 추출\n",
    "filtered_intention_idx = []\n",
    "for idx, text in enumerate(intention_map):\n",
    "    if text == label[0]:\n",
    "        filtered_intention_idx.append(idx)\n",
    "\n",
    "len(filtered_intention_idx), filtered_intention_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disease_category                                                 응급질환\n",
       "disease_name                                                       두통\n",
       "intention                                                          증상\n",
       "question            두통의 증상 중 하나로 머리에 맥박이 뛰는 것처럼 일정한 시간차로 통증이 느껴진다면...\n",
       "Name: 1481, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intention이 증상인지 확인\n",
    "question_df.iloc[1481]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. 임베딩한 질문과 가장 유사한 질문을 추출한 질문 벡터 DB에서 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'c'], dtype='<U1')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "li = [0, 2]\n",
    "a[li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링된 인덱스에 해당하는 부분만 벡터 DB로 재생성\n",
    "filtered_index = faiss.IndexFlatL2(dimension) # 차원 설정\n",
    "filtered_index.add(np.array(embeddings)[filtered_intention_idx]) # np.array는 리스트를 리스트로 감싸서 필터링 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 생성된 벡터 DB 개수\n",
    "filtered_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_distances, fi_indicies = filtered_index.search(question_vec, 5)  # 쿼리 벡터와 가장 유사한(거리가 가까운) 벡터 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[117.60936 , 119.1344  , 123.9859  , 126.939644, 135.4975  ]],\n",
       "       dtype=float32),\n",
       " array([[750, 402,  40, 705, 688]], dtype=int64))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_distances, fi_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 새로운 벡터 DB에서 나온 인덱스는 새로운 DB 기준의 인덱스 이므로 원래 벡터 DB에 해당하는 인덱스로 변환해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([750, 402,  40, 705, 688], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_indicies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2236, 1886, 1521, 2191, 2173]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_top_idx = [filtered_intention_idx[i] for i in fi_indicies[0]]\n",
    "filtered_top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 거리: 117.61 \n",
      "질문: 두통은 머리의 어떤 부위에서 느껴지는 통증을 말하는 건가요?\n",
      "벡터 거리: 119.13 \n",
      "질문: 두통과 비슷한 증상이 나타나면 머리가 찌릿한 증상인가요?\n",
      "벡터 거리: 123.99 \n",
      "질문: 두통이 발생하면 머리가 찌릿하거나 따끔거리는 것은 정상인가요?\n",
      "벡터 거리: 126.94 \n",
      "질문: 두통처럼 머리가 찌릿한 증상은 어떠한 증상으로 나타날 수 있나요?\n",
      "벡터 거리: 135.50 \n",
      "질문: 두통으로 인해 머리가 찌릿거리거나 따끔거리는 것이 일반적으로 나타나나요?\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(filtered_top_idx):\n",
    "    # print(j, i)\n",
    "    print(f'벡터 거리: {fi_distances[0][j]:.2f} \\n질문: {question_map[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 선별된 질문중 최종 질문 하나 선택\n",
    "- 질문들의 중앙값을 구하고 해당 중앙값과 가장 유사한 질문 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array([[ 1.3201225 , -1.0341312 , -0.44613767, ..., -0.49889427,\n",
       "         -0.20969974,  0.8118966 ],\n",
       "        [ 1.0746241 , -1.7189268 , -0.4708654 , ..., -0.24966142,\n",
       "         -0.52945894,  1.5034736 ],\n",
       "        [ 1.1678019 , -1.5875348 , -0.31792077, ..., -0.7242837 ,\n",
       "         -0.34021705,  0.8577244 ],\n",
       "        [ 0.9269832 , -1.3945224 , -0.39550015, ..., -0.12050028,\n",
       "         -0.44697592,  1.337872  ],\n",
       "        [ 1.0464588 , -1.7999834 , -0.51493835, ..., -0.03640296,\n",
       "         -0.29288203,  1.149456  ]], dtype=float32))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[filtered_top_idx]), embeddings[filtered_top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07462406e+00, -1.58753479e+00, -4.46137667e-01, -2.20732063e-01,\n",
       "       -3.41445535e-01,  6.84358597e-01, -6.43548444e-02, -7.32126236e-01,\n",
       "        7.39271045e-01,  6.40443325e-01, -5.13523281e-01, -3.40488851e-01,\n",
       "       -7.19415769e-02, -4.99766879e-02,  8.60560834e-01, -1.27377415e+00,\n",
       "        1.12091148e+00,  1.47653580e+00, -1.31585017e-01,  2.67702788e-01,\n",
       "       -1.57037365e+00, -8.29324543e-01,  6.44890487e-01,  1.83795333e-01,\n",
       "        1.04000163e+00, -6.99219644e-01, -4.93974060e-01, -3.43678564e-01,\n",
       "       -2.28142649e-01,  9.48988616e-01, -8.22070777e-01, -2.21551023e-02,\n",
       "       -1.08495605e+00,  1.04578674e-01,  1.30219817e-01,  6.33870602e-01,\n",
       "        1.97457328e-01, -7.98662722e-01, -6.45562336e-02, -1.61316276e+00,\n",
       "       -2.67953370e-02,  2.88116753e-01, -1.19334543e+00, -4.16712314e-01,\n",
       "       -8.72129560e-01, -5.51893413e-01, -3.91984701e-01,  7.11529076e-01,\n",
       "        3.20532560e-01,  2.44787961e-01,  9.02525246e-01, -6.59921914e-02,\n",
       "        1.21222878e+00, -3.61383677e-01,  3.22872579e-01, -1.27988672e+00,\n",
       "       -1.38039976e-01,  2.13159323e-01, -5.31282961e-01,  5.94496489e-01,\n",
       "       -1.34089267e+00,  7.13250265e-02, -6.10825837e-01, -4.46227610e-01,\n",
       "       -5.52882016e-01,  3.94512981e-01, -4.57281351e-01,  2.07548384e-02,\n",
       "       -6.16527617e-01,  8.42582583e-01,  7.66489878e-02,  1.78454638e-01,\n",
       "        7.06256747e-01, -4.50138122e-01,  6.66055322e-01, -2.06819713e-01,\n",
       "       -3.27222675e-01, -2.50302523e-01,  8.23733285e-02, -1.46387303e+00,\n",
       "       -2.60251492e-01,  1.58002973e-01,  7.51320839e-01, -1.11013733e-01,\n",
       "       -6.72872305e-01,  5.59808910e-01,  3.20358366e-01,  5.94877660e-01,\n",
       "        5.02569079e-01, -5.66934705e-01,  7.65706360e-01, -3.58909458e-01,\n",
       "       -8.61282766e-01, -4.09293734e-02,  1.24540359e-01, -2.63284892e-01,\n",
       "        1.55582651e-01,  9.19034541e-01,  7.06075191e-01,  7.82880127e-01,\n",
       "       -8.07570040e-01,  7.79735208e-01,  6.76954448e-01,  1.16107368e+00,\n",
       "        8.17465782e-01,  3.58803183e-01,  6.98374063e-02, -8.64710063e-02,\n",
       "       -4.88767296e-01, -5.75330377e-01,  5.91048121e-01,  1.03347969e+00,\n",
       "       -4.19439346e-01, -9.48448181e-02,  1.07884264e+00,  2.78370410e-01,\n",
       "       -4.53081965e-01, -3.16902936e-01,  1.05575955e+00, -2.48310700e-01,\n",
       "        6.67608142e-01,  2.57931292e-01, -1.64198244e+00, -1.54759645e-01,\n",
       "       -1.74532771e+00,  7.38730431e-01, -7.63656914e-01,  2.96842843e-01,\n",
       "       -3.24674904e-01, -1.39231920e+00, -9.25825238e-02,  1.64577812e-01,\n",
       "        2.99037308e-01, -1.01787412e+00, -1.90829322e-01, -2.55426969e-02,\n",
       "        2.21232902e-02, -3.67086768e-01,  2.55406231e-01,  1.77325040e-01,\n",
       "       -5.55348933e-01, -6.97660923e-01,  6.49038255e-01, -7.17490971e-01,\n",
       "        1.32796431e+00,  5.02344072e-01,  2.00362578e-01,  1.13836205e+00,\n",
       "       -8.22091937e-01, -3.47081125e-01,  3.67075913e-02,  1.42942250e-01,\n",
       "       -4.75051403e-01,  3.36820751e-01,  1.01863706e+00, -5.92238247e-01,\n",
       "        3.16049784e-01,  1.06472522e-01, -1.03114307e+00,  8.67316872e-02,\n",
       "       -7.53404558e-01, -6.71711504e-01, -2.32050456e-02,  2.09340692e-01,\n",
       "        2.07016468e-02,  7.67618537e-01, -8.06200132e-02, -1.24783349e+00,\n",
       "       -2.81515718e-01,  3.45872968e-01, -2.22383022e-01, -1.51539302e+00,\n",
       "        2.20217873e-04,  4.20593411e-01, -5.28527319e-01, -7.38215968e-02,\n",
       "        4.43218827e-01,  9.13000107e-02, -2.11234391e-01,  3.87455583e-01,\n",
       "       -4.10342664e-01, -1.36445868e+00,  7.12364137e-01,  5.52340865e-01,\n",
       "        1.80268005e-01,  5.27966917e-01, -1.65840521e-01,  6.25176013e-01,\n",
       "       -1.28168374e-01,  6.69260442e-01, -7.34718263e-01, -4.26961184e-01,\n",
       "        1.06348026e+00,  1.04282334e-01, -1.58918239e-02, -4.75829653e-02,\n",
       "       -4.59794670e-01, -9.91179347e-02, -7.50312433e-02, -3.38255972e-01,\n",
       "       -1.52050763e-01,  1.26619712e-01, -3.28552872e-01, -1.12107158e-01,\n",
       "        6.55542254e-01, -3.30725126e-02, -3.91210318e-01,  1.78752869e-01,\n",
       "        7.86097586e-01, -5.98454654e-01,  4.86507416e-01,  7.71337271e-01,\n",
       "       -5.23860216e-01, -7.66913518e-02, -1.20389856e-01,  3.09554517e-01,\n",
       "       -3.06528896e-01,  3.91917348e-01, -3.38357657e-01, -1.48319378e-01,\n",
       "       -2.60382652e-01,  4.54928190e-01, -5.21204829e-01,  4.10157777e-02,\n",
       "        1.90865129e-01,  1.14072871e+00, -8.32645953e-01, -1.57707945e-01,\n",
       "        6.17296286e-02, -4.08895202e-02, -5.71796060e-01, -4.83855397e-01,\n",
       "        1.17939366e-02, -1.07293606e+00, -1.24216187e+00, -6.11897148e-02,\n",
       "        7.77858555e-01,  1.72289714e-01,  1.58364189e+00,  3.06033462e-01,\n",
       "       -5.22826433e-01,  1.39855042e-01,  1.49376202e+00, -1.81413293e-01,\n",
       "       -2.57846713e-03,  7.40297258e-01,  6.67462468e-01, -4.51484472e-01,\n",
       "        2.34352902e-01, -8.41373682e-01, -1.85268950e+00,  1.37655467e-01,\n",
       "        6.39591217e-01, -2.45190129e-01,  6.08683467e-01, -1.20744550e+00,\n",
       "       -1.04358983e+00, -6.97692811e-01,  1.30691397e+00, -8.67201567e-01,\n",
       "        5.07217884e-01,  4.93926648e-03, -2.37927377e-01,  1.34689793e-01,\n",
       "       -4.50031608e-02, -8.73337448e-01,  7.13789523e-01,  5.44133067e-01,\n",
       "       -1.00738275e+00, -3.77764106e-01, -8.42661619e-01, -1.11409307e+00,\n",
       "       -2.86211342e-01, -2.65368730e-01,  6.43719971e-01, -7.33515799e-01,\n",
       "       -4.35289264e-01,  6.81297258e-02,  7.28357792e-01,  6.36804879e-01,\n",
       "       -4.14980263e-01, -5.73153317e-01, -2.89159358e-01,  1.12533796e+00,\n",
       "        3.59012514e-01,  4.08649296e-01,  3.86286199e-01,  9.65257943e-01,\n",
       "       -4.04132843e-01,  3.74362677e-01, -8.90473664e-01, -1.00656652e+00,\n",
       "        1.11623728e+00, -1.01189661e+00, -5.51809192e-01,  7.02751577e-01,\n",
       "        3.39847416e-01,  1.96593130e+00, -3.88384312e-01, -2.19139189e-01,\n",
       "        1.09313178e+00, -6.39552772e-01, -9.49779212e-01,  7.08266914e-01,\n",
       "        8.33791733e-01,  3.35642755e-01, -1.70964107e-01, -1.38588771e-01,\n",
       "        2.15814680e-01, -1.08208942e+00,  6.47036374e-01, -7.54437625e-01,\n",
       "       -6.79399073e-02, -1.26822579e+00,  4.65903282e-01, -1.73800242e+00,\n",
       "        1.17473349e-01,  8.82172063e-02,  8.08665276e-01,  3.90078247e-01,\n",
       "       -2.58839503e-02,  5.93379736e-01, -4.61499125e-01,  4.92934018e-01,\n",
       "       -9.46912169e-02,  3.24137479e-01,  5.53671360e-01,  6.93363190e-01,\n",
       "        6.07999802e-01,  1.25296187e+00, -1.64173737e-01,  4.35844153e-01,\n",
       "       -4.48263027e-02,  9.40521955e-02,  2.02997312e-01, -1.91118792e-01,\n",
       "        1.13089609e+00,  4.64114904e-01,  1.29298151e+00, -5.70499957e-01,\n",
       "        2.90917337e-01,  9.64050293e-01, -3.25122885e-02, -7.92780101e-01,\n",
       "       -1.52623796e+00, -1.77841708e-02, -1.41264749e+00,  5.87857425e-01,\n",
       "        3.66041996e-02, -3.37659776e-01, -2.20858192e+00, -7.74283558e-02,\n",
       "        6.42120659e-01,  4.22922552e-01, -1.65419415e-01,  5.18622398e-01,\n",
       "       -1.80660456e-01,  2.70333558e-01,  9.27990377e-01,  6.83589578e-01,\n",
       "        1.21664548e+00, -7.65567794e-02, -3.20695550e-03, -7.98334002e-01,\n",
       "       -1.02843988e+00,  2.20941216e-01, -5.84425211e-01, -9.08838362e-02,\n",
       "        7.42136478e-01,  1.60528794e-01, -7.38138199e-01,  7.16606617e-01,\n",
       "       -6.20567977e-01,  2.94407874e-01, -8.88376057e-01, -7.83312678e-01,\n",
       "        1.19486056e-01,  2.68002361e-01,  2.28560939e-01,  5.02573773e-02,\n",
       "       -8.32061529e-01, -5.45706093e-01,  1.33437559e-01,  5.65855980e-01,\n",
       "        6.59966394e-02, -2.75247246e-01, -2.68707275e-01,  3.17447424e-01,\n",
       "        1.24280143e+00, -9.32460874e-02,  9.35294151e-01,  1.00312662e+00,\n",
       "        7.89405167e-01, -3.98297250e-01, -3.63055736e-01,  2.82278925e-01,\n",
       "        1.09931874e+00, -3.10139328e-01, -8.95743817e-02, -6.31165028e-01,\n",
       "       -2.50731885e-01, -3.45544755e-01, -9.15904462e-01,  3.81517321e-01,\n",
       "        1.57578182e+00,  5.85602522e-01,  6.10805809e-01, -6.39574766e-01,\n",
       "        3.23351979e-01,  9.17035639e-02,  1.45587826e+00,  1.19781889e-01,\n",
       "        9.85212848e-02, -1.40033215e-01,  7.08910406e-01, -3.84426713e-01,\n",
       "       -4.02103573e-01,  1.22315109e+00, -1.49202019e-01, -4.58421171e-01,\n",
       "        1.10230660e+00, -6.23979568e-01,  8.87813449e-01,  4.77683514e-01,\n",
       "       -3.16976815e-01, -4.65623975e-01,  1.28730106e+00,  2.50660509e-01,\n",
       "        3.51123810e-01, -8.92449260e-01, -9.41307783e-01,  1.80881456e-01,\n",
       "       -9.10442710e-01,  1.08805429e-02,  7.80087650e-01, -1.15329552e+00,\n",
       "       -9.21078801e-01, -1.06297798e-01,  3.46608430e-01, -4.76132393e-01,\n",
       "       -7.52442479e-01, -4.92883414e-01,  1.13321316e+00,  2.84181118e-01,\n",
       "        4.14879799e-01,  4.61384237e-01, -8.58247746e-03,  2.20462412e-01,\n",
       "       -1.47466087e+00,  8.97414625e-01, -2.95692027e-01,  7.37368107e-01,\n",
       "       -8.29433382e-01,  4.47791398e-01,  2.49796808e-01, -3.43640268e-01,\n",
       "       -8.50442886e-01, -8.84946525e-01, -5.50402664e-02, -4.05863494e-01,\n",
       "        7.53342688e-01,  4.86978859e-01,  1.67716518e-01,  4.14476573e-01,\n",
       "       -3.83058697e-01,  1.66289777e-01,  2.51287013e-01,  4.68795151e-01,\n",
       "        6.65203452e-01, -7.56010592e-01,  8.89549911e-01, -6.56949997e-01,\n",
       "        2.03638747e-01,  2.93252707e-01, -7.38382578e-01,  2.85913408e-01,\n",
       "        6.98082626e-01,  9.54188183e-02,  3.96977156e-01,  4.96284485e-01,\n",
       "        5.36034048e-01,  4.82919008e-01,  2.22400695e-01, -5.48720658e-01,\n",
       "       -1.65075064e-02,  5.14133811e-01, -6.29609048e-01, -5.49638346e-02,\n",
       "       -3.67191315e-01, -5.48616111e-01,  5.58638930e-01, -4.56330776e-01,\n",
       "       -1.08725405e+00, -2.35367253e-01, -7.09285259e-01, -1.01553679e+00,\n",
       "        1.04180908e+00, -3.49453576e-02, -1.06502257e-01, -1.71555981e-01,\n",
       "       -9.57929134e-01,  1.26286972e+00, -5.43861687e-01,  5.40200062e-02,\n",
       "        4.69326586e-01,  1.62866101e-01, -8.71997774e-02,  2.52124757e-01,\n",
       "        2.24900916e-01, -1.97673813e-01,  9.07878637e-01, -9.52044129e-01,\n",
       "       -5.39966345e-01, -9.85155225e-01, -2.89783329e-01,  4.13322836e-01,\n",
       "        1.18658042e+00,  1.33332387e-01, -5.91740966e-01,  2.56389618e-01,\n",
       "        1.68900371e-01,  1.11582309e-01,  5.06337345e-01,  6.04193807e-01,\n",
       "       -1.06460638e-01, -1.86908513e-01, -3.81218016e-01, -5.05264938e-01,\n",
       "        2.76590645e-01, -1.16438448e-01, -1.14366865e+00, -1.18052161e+00,\n",
       "       -2.68577188e-01, -4.53823626e-01, -7.52287924e-01,  1.38150185e-01,\n",
       "        6.30762875e-01, -2.42442057e-01,  1.07264996e+00,  2.00638399e-01,\n",
       "       -7.11840689e-01,  8.51865530e-01, -1.25202894e-01, -2.29289830e-01,\n",
       "       -1.33366376e-01, -1.51178047e-01,  1.06683671e+00, -1.04622090e+00,\n",
       "       -5.21161139e-01, -4.54296112e-01,  1.20060301e+00,  3.05998117e-01,\n",
       "       -6.21386245e-02,  4.72020209e-01, -1.70062363e+00, -4.04137939e-01,\n",
       "        1.05234230e+00, -8.28455508e-01, -7.45135844e-01,  8.20383072e-01,\n",
       "        1.15614258e-01,  2.14951649e-01, -1.71509802e+00,  6.21887863e-01,\n",
       "       -3.04717004e-01, -4.56220120e-01,  2.91755915e-01, -4.25802767e-01,\n",
       "        1.82392314e-01, -7.47226402e-02,  6.75535142e-01, -1.50567323e-01,\n",
       "        5.61836541e-01, -3.03759649e-02,  4.66864735e-01,  4.86606598e-01,\n",
       "       -1.40194154e+00, -1.65314525e-01,  5.87469578e-01,  8.03973854e-01,\n",
       "        1.46436322e+00,  1.16877866e+00, -4.04032499e-01, -5.56374371e-01,\n",
       "        4.06742185e-01,  1.18276358e+00,  7.63383150e-01, -8.68833244e-01,\n",
       "       -9.57753837e-01, -5.12460351e-01,  7.86791503e-01,  1.83638781e-01,\n",
       "        2.69982427e-01,  8.49690497e-01, -1.51648045e-01, -5.34562878e-02,\n",
       "       -7.40891039e-01,  3.93757403e-01,  2.34148234e-01,  2.41222307e-01,\n",
       "       -9.25594941e-02,  4.87646103e-01,  1.05033040e+00,  7.84627199e-01,\n",
       "        3.38446438e-01,  9.90897059e-01, -6.51276946e-01, -1.57830045e-01,\n",
       "        3.68006170e-01, -7.29986846e-01,  5.68464041e-01, -3.79323363e-01,\n",
       "       -7.37855256e-01, -9.92758274e-01, -1.21350527e-01, -4.32587415e-02,\n",
       "       -2.14048903e-02, -2.29220465e-02, -6.98248684e-01, -1.20200086e+00,\n",
       "        2.11290866e-01, -2.55739331e-01,  9.32449996e-01,  4.07775104e-01,\n",
       "        9.80998993e-01, -1.86083660e-01,  1.18847266e-01, -4.55861092e-01,\n",
       "       -5.23392916e-01, -1.17831670e-01,  1.18466294e+00, -8.56426775e-01,\n",
       "        3.17518979e-01, -5.00144660e-01, -7.89581418e-01, -4.11896408e-01,\n",
       "       -5.49727559e-01,  5.98508358e-01,  4.28928584e-01,  1.11521769e+00,\n",
       "        1.92820475e-01, -7.55034745e-01,  4.87283200e-01, -7.62334108e-01,\n",
       "        3.89445536e-02, -1.30489159e+00, -4.33322608e-01, -7.66639039e-02,\n",
       "        5.09245932e-01, -3.44301194e-01, -9.34087455e-01, -1.21044382e-01,\n",
       "        1.74373224e-01,  8.32522929e-01, -1.94941178e-01, -9.09740031e-01,\n",
       "       -2.43804112e-01,  1.72220993e+00,  1.44359320e-01,  4.26125973e-01,\n",
       "       -1.06787431e+00,  1.30685878e+00,  7.85336196e-01, -1.64096430e-01,\n",
       "        4.12103012e-02,  9.73323166e-01, -1.71684578e-01, -7.02761769e-01,\n",
       "        9.30560410e-01,  9.47891399e-02, -7.37617433e-01,  4.21811789e-01,\n",
       "       -3.94082546e-01, -9.68030095e-01,  5.87187827e-01,  9.04996917e-02,\n",
       "       -1.10176528e+00,  8.82313251e-01, -8.95377576e-01,  4.67046022e-01,\n",
       "        6.42140090e-01, -5.08245945e-01,  1.06876180e-01,  1.34949699e-01,\n",
       "       -6.78577662e-01, -3.63568038e-01,  9.23666954e-02,  4.05025542e-01,\n",
       "        3.31201106e-02, -8.41507077e-01,  1.08106649e+00, -1.22597420e+00,\n",
       "        1.04402840e-01,  1.01489522e-01,  4.52302128e-01, -4.44324650e-02,\n",
       "        2.18547687e-01,  1.48864698e+00, -9.59205747e-01,  4.60134357e-01,\n",
       "       -3.41664739e-02,  6.80060863e-01, -4.55554038e-01, -1.12589002e+00,\n",
       "        1.39500767e-01,  2.77004331e-01, -1.90469861e-01, -7.02525139e-01,\n",
       "        8.09695840e-01, -8.76406193e-01,  2.50345647e-01,  2.39900962e-01,\n",
       "        3.33842747e-02,  1.91196010e-01, -5.86107016e-01,  1.92997187e-01,\n",
       "       -8.08486402e-01, -4.07906890e-01, -8.82224917e-01,  5.27191758e-01,\n",
       "       -1.44699980e-02, -2.20869452e-01,  4.91051853e-01,  6.24274239e-02,\n",
       "        2.94996709e-01,  4.88949239e-01, -6.36191547e-01,  7.74527013e-01,\n",
       "       -9.67210889e-01, -8.44672397e-02, -2.15668783e-01,  8.39403689e-01,\n",
       "       -3.50353628e-01,  3.27020913e-01,  1.03113770e+00,  1.23347604e+00,\n",
       "       -3.83207262e-01,  4.57384855e-01, -5.70933163e-01,  1.38518855e-01,\n",
       "        1.77036542e-02,  4.53860283e-01,  8.21731865e-01,  6.82087839e-01,\n",
       "       -3.75228114e-02,  5.36782518e-02,  1.00409472e+00, -5.74075699e-01,\n",
       "       -8.98408592e-01,  2.61591464e-01, -1.62607598e+00,  9.85119522e-01,\n",
       "        7.68279195e-01, -1.18917727e+00, -1.12312794e+00, -6.95992529e-01,\n",
       "       -1.21110177e+00,  1.91247180e-01,  7.70681441e-01, -7.92888105e-01,\n",
       "        7.23106638e-02,  1.29540965e-01, -1.33409902e-01,  3.84356193e-02,\n",
       "        1.14544407e-01, -2.49661416e-01, -3.40217054e-01,  1.14945602e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5개 벡터의 중앙값 구하기\n",
    "filtered_vectors = embeddings[filtered_top_idx]\n",
    "median_vector = np.median(filtered_vectors, axis=0)\n",
    "median_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유사도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vec = embeddings[filtered_top_idx]\n",
    "final_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모양을 맞춰줘야함\n",
    "reshape_median_vec = median_vector.reshape(1, -1)\n",
    "reshape_median_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도가 가장 높은 인덱스:  2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 중앙벡터와 5개의 최종 벡터와의 코사인 유사도 계산\n",
    "cos_similarities = cosine_similarity(reshape_median_vec, final_vec)\n",
    "most_similar_idx = np.argmax(cos_similarities) # np.argmax: numpy 배열에서 가장 큰 값의 인덱스 반환\n",
    "print('유사도가 가장 높은 인덱스: ', most_similar_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1521"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 벡터 DB에서 찾아야할 인덱스 번호\n",
    "final_idx = filtered_top_idx[most_similar_idx]\n",
    "final_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 질문: 오른쪽 머리에서 찌릿하는 두통은 뭔가요?\n",
      "최종 선택 질문: 두통이 발생하면 머리가 찌릿하거나 따끔거리는 것은 정상인가요?\n"
     ]
    }
   ],
   "source": [
    "print(f'처음 질문: {question}\\n최종 선택 질문: {question_map[final_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. fine tunning한 LLM 모델에 최종 사용하기 위해 클래스로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class SelectOneQuestion:\n",
    "\n",
    "    def __init__(self, csvfile, scaler, pca, rf_model, embed_model, labelencoder, question):\n",
    "        self.csvfile = csvfile\n",
    "        self.scaler = scaler\n",
    "        self.pca = pca\n",
    "        self.rfmodel = rf_model\n",
    "        self.embed_model = embed_model\n",
    "        self.le = labelencoder\n",
    "        \n",
    "        # 변수 초기화\n",
    "        self.embeddings = None\n",
    "        self.dimension = None\n",
    "\n",
    "    # 벡터 DB 만드는 메서드\n",
    "    def make_vec_db(self):\n",
    "\n",
    "        # 2문장씩 문장을 나누는 함수\n",
    "        def split_sentences(sentences, max_sentences=2):\n",
    "            sentence_list = re.split(r'(?<=[.!?]) +', sentences) # 문장을 마침표, 느낌표, 물음표 뒤에오는 공백을 기준으로 나눔(공백 뒤에 +를 붙여서 하나이상을 의미)\n",
    "            chunks = []\n",
    "            for i in range(0, len(sentence_list), max_sentences): # max_sentences 간격만큼\n",
    "                chunks.append(' '.join(sentence_list[i:i + max_sentences])) # join으로 두개의 문장 공백으로 연결, 리스트 대신 문자열로 생성됨.\n",
    "            \n",
    "            return chunks\n",
    "\n",
    "        # chunks 속 2문장씩 결합되어있는 문장들의 길이가 너무 긴 경우 나눠주기 위함.\n",
    "        def split_text_with_recursive_splitter(text, chunk_size=300, chunk_overlap=50):\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size = chunk_size, # 최대 몇자로 설정할지\n",
    "                chunk_overlap = chunk_overlap # 50개 겹쳐서 시작\n",
    "            )\n",
    "\n",
    "            return text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "        question_df = pd.read_csv(self.csvfile, index_col=0)\n",
    "        question_list = question_df['question'].tolist()\n",
    "        intention_list = question_df['intention'].tolist()\n",
    "        le_intention = self.le.fit_transform(question_df['intention'])\n",
    "\n",
    "\n",
    "        final_chunk = []\n",
    "        question_map = []\n",
    "        intention_map = []\n",
    "\n",
    "        for question in question_list:\n",
    "            # 2문장씩 나누기\n",
    "            sentence_chunk = split_sentences(question, max_sentences=2)\n",
    "\n",
    "            # 긴 문장 쪼개기\n",
    "            for chunk in sentence_chunk:\n",
    "                split_chunks = split_text_with_recursive_splitter(chunk, chunk_size=300, chunk_overlap=50)\n",
    "                final_chunk.extend(split_chunks)\n",
    "\n",
    "                # 분할된 chunk마다 원본 맵핑\n",
    "                question_map.extend([question] * len(split_chunks))\n",
    "                # 분할된 chunk마다 질문 의도 맵핑\n",
    "                intention = question_df[question_df['question'] == question]['intention'].values[0] # series 형태여서 values로 텍스트만 가져옴.\n",
    "                intention_map.extend([intention] * len(split_chunks))\n",
    "\n",
    "        # 임베딩\n",
    "        self.embeddings = self.embed_model.encode(final_chunk)\n",
    "        \n",
    "        self.dimension = self.embeddings.shape[1] # 임베딩 벡터의 차원\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)  # L2 거리(유클리드 거리) 기반으로 검색을 수행하도록 설정(인덱스 생성)\n",
    "        self.index.add(np.array(self.embeddings)) # 각 벡터를 L2 거리 기반 인덱스에 추가하고, 검색을 빠르게 할 수 있도록 데이터 구조를 최적화\n",
    "\n",
    "        return intention_map, question_map\n",
    "\n",
    "    def search_one_question(self, question, intention_map, question_map):\n",
    "        \n",
    "        #질문 임베딩\n",
    "        question_vec = self.embed_model.encode([question], batch_size=32)\n",
    "\n",
    "        question_ss = self.scaler.transform(question_vec)\n",
    "        question_pca = self.pca.transform(question_ss)\n",
    "        result = self.rfmodel.predict(question_pca)\n",
    "\n",
    "        label = self.le.inverse_transform([result])\n",
    "\n",
    "        # 분류 결과와 같은 증상의 인덱스만 추출\n",
    "        filtered_intention_idx = []\n",
    "        for idx, text in enumerate(intention_map):\n",
    "            if text == label[0]:\n",
    "                filtered_intention_idx.append(idx)\n",
    "\n",
    "        # 필터링된 인덱스에 해당하는 부분만 벡터 DB로 재생성\n",
    "        filtered_index = faiss.IndexFlatL2(self.dimension) # 차원 설정\n",
    "        filtered_index.add(np.array(self.embeddings)[filtered_intention_idx]) # np.array는 리스트를 리스트로 감싸서 필터링 가능\n",
    "\n",
    "        fi_distances, fi_indicies = filtered_index.search(question_vec, 5)  # 쿼리 벡터와 가장 유사한(거리가 가까운) 벡터 검색\n",
    "        filtered_top_idx = [filtered_intention_idx[i] for i in fi_indicies[0]]\n",
    "        \n",
    "        # 5개 벡터의 중앙값 구하기\n",
    "        filtered_vectors = self.embeddings[filtered_top_idx]\n",
    "        median_vector = np.median(filtered_vectors, axis=0)\n",
    "        final_vec = self.embeddings[filtered_top_idx]\n",
    "\n",
    "        # 모양을 맞춰줘야함\n",
    "        reshape_median_vec = median_vector.reshape(1, -1)\n",
    "\n",
    "        # 중앙벡터와 5개의 최종 벡터와의 코사인 유사도 계산\n",
    "        cos_similarities = cosine_similarity(reshape_median_vec, final_vec)\n",
    "        most_similar_idx = np.argmax(cos_similarities) # np.argmax: numpy 배열에서 가장 큰 값의 인덱스 반환\n",
    "\n",
    "        final_idx = filtered_top_idx[most_similar_idx]\n",
    "        \n",
    "        return question_map[final_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성한 Class를 .py 형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SelectQuestion.py', 'w') as f:\n",
    "    f.write('''\n",
    "class SelectOneQuestion:\n",
    "\n",
    "    def __init__(self, csvfile, scaler, pca, rf_model, embed_model, labelencoder, question):\n",
    "        self.csvfile = csvfile\n",
    "        self.scaler = scaler\n",
    "        self.pca = pca\n",
    "        self.rfmodel = rf_model\n",
    "        self.embed_model = embed_model\n",
    "        self.le = labelencoder\n",
    "        \n",
    "        # 변수 초기화\n",
    "        self.embeddings = None\n",
    "        self.dimension = None\n",
    "\n",
    "    # 벡터 DB 만드는 메서드\n",
    "    def make_vec_db(self):\n",
    "\n",
    "        # 2문장씩 문장을 나누는 함수\n",
    "        def split_sentences(sentences, max_sentences=2):\n",
    "            sentence_list = re.split(r'(?<=[.!?]) +', sentences) # 문장을 마침표, 느낌표, 물음표 뒤에오는 공백을 기준으로 나눔(공백 뒤에 +를 붙여서 하나이상을 의미)\n",
    "            chunks = []\n",
    "            for i in range(0, len(sentence_list), max_sentences): # max_sentences 간격만큼\n",
    "                chunks.append(' '.join(sentence_list[i:i + max_sentences])) # join으로 두개의 문장 공백으로 연결, 리스트 대신 문자열로 생성됨.\n",
    "            \n",
    "            return chunks\n",
    "\n",
    "        # chunks 속 2문장씩 결합되어있는 문장들의 길이가 너무 긴 경우 나눠주기 위함.\n",
    "        def split_text_with_recursive_splitter(text, chunk_size=300, chunk_overlap=50):\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size = chunk_size, # 최대 몇자로 설정할지\n",
    "                chunk_overlap = chunk_overlap # 50개 겹쳐서 시작\n",
    "            )\n",
    "\n",
    "            return text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "        question_df = pd.read_csv(self.csvfile, index_col=0)\n",
    "        question_list = question_df['question'].tolist()\n",
    "        intention_list = question_df['intention'].tolist()\n",
    "        le_intention = self.le.fit_transform(question_df['intention'])\n",
    "\n",
    "\n",
    "        final_chunk = []\n",
    "        question_map = []\n",
    "        intention_map = []\n",
    "\n",
    "        for question in question_list:\n",
    "            # 2문장씩 나누기\n",
    "            sentence_chunk = split_sentences(question, max_sentences=2)\n",
    "\n",
    "            # 긴 문장 쪼개기\n",
    "            for chunk in sentence_chunk:\n",
    "                split_chunks = split_text_with_recursive_splitter(chunk, chunk_size=300, chunk_overlap=50)\n",
    "                final_chunk.extend(split_chunks)\n",
    "\n",
    "                # 분할된 chunk마다 원본 맵핑\n",
    "                question_map.extend([question] * len(split_chunks))\n",
    "                # 분할된 chunk마다 질문 의도 맵핑\n",
    "                intention = question_df[question_df['question'] == question]['intention'].values[0] # series 형태여서 values로 텍스트만 가져옴.\n",
    "                intention_map.extend([intention] * len(split_chunks))\n",
    "\n",
    "        # 임베딩\n",
    "        self.embeddings = self.embed_model.encode(final_chunk)\n",
    "        \n",
    "        self.dimension = self.embeddings.shape[1] # 임베딩 벡터의 차원\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)  # L2 거리(유클리드 거리) 기반으로 검색을 수행하도록 설정(인덱스 생성)\n",
    "        self.index.add(np.array(self.embeddings)) # 각 벡터를 L2 거리 기반 인덱스에 추가하고, 검색을 빠르게 할 수 있도록 데이터 구조를 최적화\n",
    "\n",
    "        return intention_map, question_map\n",
    "\n",
    "    def search_one_question(self, question, intention_map, question_map):\n",
    "        \n",
    "        #질문 임베딩\n",
    "        question_vec = self.embed_model.encode([question], batch_size=32)\n",
    "\n",
    "        question_ss = self.scaler.transform(question_vec)\n",
    "        question_pca = self.pca.transform(question_ss)\n",
    "        result = self.rfmodel.predict(question_pca)\n",
    "\n",
    "        label = self.le.inverse_transform([result])\n",
    "\n",
    "        # 분류 결과와 같은 증상의 인덱스만 추출\n",
    "        filtered_intention_idx = []\n",
    "        for idx, text in enumerate(intention_map):\n",
    "            if text == label[0]:\n",
    "                filtered_intention_idx.append(idx)\n",
    "\n",
    "        # 필터링된 인덱스에 해당하는 부분만 벡터 DB로 재생성\n",
    "        filtered_index = faiss.IndexFlatL2(self.dimension) # 차원 설정\n",
    "        filtered_index.add(np.array(self.embeddings)[filtered_intention_idx]) # np.array는 리스트를 리스트로 감싸서 필터링 가능\n",
    "\n",
    "        fi_distances, fi_indicies = filtered_index.search(question_vec, 5)  # 쿼리 벡터와 가장 유사한(거리가 가까운) 벡터 검색\n",
    "        filtered_top_idx = [filtered_intention_idx[i] for i in fi_indicies[0]]\n",
    "        \n",
    "        # 5개 벡터의 중앙값 구하기\n",
    "        filtered_vectors = self.embeddings[filtered_top_idx]\n",
    "        median_vector = np.median(filtered_vectors, axis=0)\n",
    "        final_vec = self.embeddings[filtered_top_idx]\n",
    "\n",
    "        # 모양을 맞춰줘야함\n",
    "        reshape_median_vec = median_vector.reshape(1, -1)\n",
    "\n",
    "        # 중앙벡터와 5개의 최종 벡터와의 코사인 유사도 계산\n",
    "        cos_similarities = cosine_similarity(reshape_median_vec, final_vec)\n",
    "        most_similar_idx = np.argmax(cos_similarities) # np.argmax: numpy 배열에서 가장 큰 값의 인덱스 반환\n",
    "\n",
    "        final_idx = filtered_top_idx[most_similar_idx]\n",
    "        \n",
    "        return question_map[final_idx]\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from SelectQuestion import SelectOneQuestion\n",
    "\n",
    "# 저장된 RandomForestClassifier 모델 불러오기\n",
    "rf_model = joblib.load('./headache_randomforest.pkl')\n",
    "scaler = joblib.load('./headache_scaler.pkl')\n",
    "pca = joblib.load('./headache_pca.pkl')\n",
    "\n",
    "csvfile = \"두통 질문.csv\"\n",
    "embed_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "labelencoder = LabelEncoder()\n",
    "question = '오른쪽 머리에서 찌릿하는 두통은 뭔가요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 생성\n",
    "select_question = SelectOneQuestion(csvfile, scaler, pca, rf_model, embed_model, labelencoder, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 DB 만들기\n",
    "intention_map, question_map = obj.make_vec_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paris\\miniconda3\\envs\\ds_study\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'두통이 발생하면 머리가 찌릿하거나 따끔거리는 것은 정상인가요?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = obj.search_one_question(question, intention_map, question_map)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
